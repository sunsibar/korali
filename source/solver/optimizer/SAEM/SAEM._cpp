#include "solver/optimizer/SAEM/SAEM.hpp"
#include "problem/problem.hpp"
#include "experiment/experiment.hpp"
#include "conduit/conduit.hpp"

#include <string>
#include <stdio.h>
#include <vector>

void korali::solver::optimizer::SAEM::initialize()
{

  if( _k->_problem->getType() != "Evaluation/Bayesian/Inference/Latent")
    korali::logError("SAEM can only optimize problems of type 'Evaluation/Bayesian/Inference/Latent' .\n");

  N = _k->_variables.size();
  NLatent = _k->_latentVariableIndices.size();

  if (_k->_currentGeneration > 0) return;

  for (size_t i = 0; i < N; i++)
    if( std::isfinite(_k->_variables[i]->_initialValue) == false )
      korali::logError("Initial Value of variable \'%s\' not defined (no defaults can be calculated).\n", _k->_variables[i]->_name.c_str());

  /* Create a sampler if none is given */

  /* */


  _currentX.resize( N, 0.0 );
  for (size_t i = 0; i < N; i++)
    _currentX[i] = _k->_variables[i]->_initialValue;

  _bestX = _currentX;
  //_delta.resize( N, _delta0 );
  //_currentGradient.resize( N, 0);
  //_previousGradient.resize( N, 0.0 );

  _bestEvaluation = korali::Inf;
  //_xDiff = korali::Inf;
  //_maxStallCounter = 0;
  //_normPreviousGradient = korali::Inf;
  //_previousEvaluation   = korali::Inf;
}





void korali::solver::optimizer::SAEM::initializeSampler( void)
{

    /*
    probability to sample from:
    p(d, z | theta) * p(theta) -- that's the (log-)posterior
    - use a "Custom" bayesian problem, with our log-posterior as "Likelihood Model" , but with the current values for theta inserted.

    */

    /* Create one sampling experiment for each latent variable.
      -- Edit: Create one sampling experiment over all, after all the latent vars are correlated / have a joint distrib.
        Todo: does it make sense to re-create these experiments at every E-M step? Otherwise, how
            to automatically update the initial mean and the distribution function of the sampling experiments?*/
    //std::vector<korali::Experiment> eList;
    //for (size_t i=0; i <NLatent; i++){

         auto k = korali::Engine();
         auto e = korali::Experiment();
         //auto p = heat2DInit(&argc, &argv);

         // Based on tutorial a2-sampling
         e["Problem"]["Type"] = "Evaluation/Direct/Basic"
         e["Problem"]["Objective Function"] = [_currentHyperparameters](korali::sample s) -> void {
                        if (! sample.contains("Latent Variables")){
                            korali::logError("You try to evaluate the likelihood without passing values for the latent variables to the sample.\n"); }
                        s["Hyperparameters"] = _currentHyperparameters;
                        // Ugly? & Probably doesnt work
                        s->_problem->_evaluateLogLikelihood();
                        }

        for (size_t i=0; i <NLatent; i++){

             size_t idx = _latentVariableIndices[i];
             std::string varName = _k->_variables[idx]->_name;

             if (_k->_currentGeneration == 0){
                std::double previousSampleMean =  _previousLatentSampleMeans[i]; // TODO: Check, do I need a vector of vectors instead?
             } else {
                std::double previousSampleMean = _k->variables[idx]->_initialValue;
             }
            // Defining problem's variables
            e["Variables"][i]["Name"] = varName;
            e["Variables"][i]["Initial Mean"] = previousSampleMean;
            e["Variables"][i]["Initial Standard Deviation"] = 1.0;
        }

        // Configuring the MCMC sampler parameters
        e["Solver"]["Type"]  = "Sampler/MCMC";
        e["Solver"]["Burn In"] = 500;
        e["Solver"]["Termination Criteria"]["Max Samples"] = 5000;

        // Configuring output settings
        e["Results"]["Frequency"] = 500;
        e["Console"]["Frequency"] = 500;
        e["Console"]["Verbosity"] = "Detailed";

        // Todo: I don't think a result path is needed (and it'd need a step id in the pathname as well)
        e["Results"]["Path"] = "setup/results_phase_1/" + "0"*(3 - str(i).length()) +  std:to_string(i);




        //e["Problem"]["Type"] = "Evaluation/Direct/Basic";
        //e["Problem"]["Objective Function"] = &model;


      //  eList.push_back(e);

    // }
}

/*void korali::solver::optimizer::Rprop::evaluateFunctionAndGradient( void )
{
  int Ns = 1;
  // Initializing Sample Evaluation
  std::vector<korali::Sample> samples(Ns);
  for (size_t i = 0; i < Ns; i++){
    samples[i]["Operation"]  = "Basic Evaluation";
    samples[i]["Parameters"] = _currentX;
    samples[i]["Sample Id"]  = i;
    _modelEvaluationCount++;
    korali::_conduit->start(samples[i]);
  }

  // Waiting for samples to finish
  korali::_conduit->waitAll(samples);

  // Processing results
  // The 'minus' is there because we want Rprop to do Maximization be default.
  for (size_t i = 0; i < Ns; i++){
    _currentEvaluation = samples[i]["Evaluation"];
    _currentEvaluation = -_currentEvaluation;
    for( size_t j=0; j<N; j++){
      _currentGradient[j] = samples[i]["Gradient"][j];
      _currentGradient[j] = -_currentGradient[j];
    }
  }
}*/

void korali::solver::optimizer::SAEM::runGeneration( void )
{

  /* E1: Sample latent variable values */


  /* E2: Update posterior probability function Q */


  /* M:  Find argmax Q(theta) */



  /* * * * * * */



  evaluateFunctionAndGradient( );

  korali::logInfo("Normal","X = [ ");
  for(size_t k=0; k<N; k++) korali::logData("Normal"," %.5le  ",_currentX[k]);
  korali::logData("Normal"," ]\n");

  korali::logInfo("Normal","F(X) = %le \n", _currentEvaluation );

  korali::logInfo("Normal","DF(X) = [ ");
  for(size_t k=0; k<N; k++) korali::logData("Normal"," %.5le  ",_currentGradient[k]);
  korali::logData("Normal"," ]\n");

  korali::logInfo("Normal","X_best = [ ");
  for(size_t k=0; k<N; k++) korali::logData("Normal"," %.5le  ",_bestX[k]);
  korali::logData("Normal"," ]\n");

  Update_iminus();

  _previousEvaluation   = _currentEvaluation;
  _previousGradient     = _currentGradient;
  _normPreviousGradient = vectorNorm(_previousGradient);

  if( _currentEvaluation < _bestEvaluation ){
    _bestEvaluation = _currentEvaluation;
    std::vector<double> tmp(N);
    for(size_t j=0; j<N; j++) tmp[j] = _bestX[j]-_currentX[j];
    _xDiff = vectorNorm( tmp );
    _bestX = _currentX;
    _maxStallCounter = 0;
  }
  else{
    _maxStallCounter++;
  }

}




void korali::solver::optimizer::SAEM::printGenerationBefore(){ return; }

void korali::solver::optimizer::SAEM::printGenerationAfter() { return; }

void korali::solver::optimizer::SAEM::finalize()  { return; }
